\documentclass[a4paper]{article}
\usepackage{a4wide,amssymb,epsfig,latexsym,multicol,array,hhline,fancyhdr}
\usepackage{vntex}
\usepackage[T5]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{float}
\usepackage{amsmath}
\usepackage{lastpage}
\usepackage[lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{enumerate}
\usepackage{color}
\usepackage{graphicx}							% Standard graphics package
\usepackage{array}
\usepackage{tabularx, caption}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{rotating}
\usepackage{graphics}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{epsfig}
\usepackage{tikz}
\usetikzlibrary{arrows,snakes,backgrounds}
\usepackage{hyperref}
\hypersetup{urlcolor=blue,linkcolor=black,citecolor=black,colorlinks=true}
%\usepackage{pstcol} 								% PSTricks with the standard color package
\usepackage{xcolor}

\usepackage{listings}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\newtheorem{theorem}{{\bf Theorem}}
\newtheorem{property}{{\bf Property}}
\newtheorem{proposition}{{\bf Proposition}}
\newtheorem{corollary}[proposition]{{\bf Corollary}}
\newtheorem{lemma}[proposition]{{\bf Lemma}}
\AtBeginDocument{\renewcommand*\contentsname{MỤC LỤC}}
\AtBeginDocument{\renewcommand*\refname{References}}
%\usepackage{fancyhdr}
\setlength{\headheight}{40pt}
\pagestyle{fancy}
\fancyhead{} % clear all header fields
\fancyhead[L]{
 \begin{tabular}{rl}
    \begin{picture}(25,15)(0,0)
    \put(0,-8){\includegraphics[width=8mm, height=8mm]{images/hcmut.png}}
    %\put(0,-8){\epsfig{width=10mm,figure=hcmut.eps}}
   \end{picture}&
	%\includegraphics[width=8mm, height=8mm]{hcmut.png} & %
	\begin{tabular}{l}
		\textbf{\bf \ttfamily Trường Đại học Bách Khoa thành phố Hồ Chí Minh}\\
		\textbf{\bf \ttfamily Khoa Khoa học và Kỹ thuật Máy tính}
	\end{tabular} 	
 \end{tabular}
}
\fancyhead[R]{
	\begin{tabular}{l}
		\tiny \bf \\
		\tiny \bf 
	\end{tabular}  }
\fancyfoot{} % clear all footer fields
\fancyfoot[L]{\scriptsize \ttfamily Bài tập lớn môn Nhập môn Trí Tuệ Nhân Tạo - Năm học 2025}
\fancyfoot[R]{\scriptsize \ttfamily Page {\thepage}/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.3pt}


%%%
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{3}
\makeatletter
\newcounter {subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection .\@alph\c@subsubsubsection}
\newcommand\subsubsubsection{\@startsection{subsubsubsection}{4}{\z@}%
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {1.5ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\newcommand*\l@subsubsubsection{\@dottedtocline{3}{10.0em}{4.1em}}
\newcommand*{\subsubsubsectionmark}[1]{}
\makeatother


\begin{document}

\begin{titlepage}
\begin{center}
ĐẠI HỌC QUỐC GIA THÀNH PHỐ HỒ CHÍ MINH \\
TRƯỜNG ĐẠI HỌC BÁCH KHOA \\
KHOA KHOA HỌC VÀ KỸ THUẬT MÁY TÍNH
\end{center}

\vspace{1cm}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=3cm]{images/hcmut.png}
\end{center}
\end{figure}

\vspace{1cm}


\begin{center}
\begin{tabular}{c}
\multicolumn{1}{l}{\textbf{{\Large NHẬP MÔN TRÍ TUỆ NHÂN TẠO (CO3061)}}}\\
~~\\
\hline
\\
\multicolumn{1}{l}{\textbf{{\Large Bài tập lớn 3}}}\\
\\
\textbf{{\Huge Chess AI Agent System}}\\
\\
\hline
\end{tabular}
\end{center}

\vspace{3cm}

\begin{table}[h]
\begin{tabular}{rrl}
\hspace{5 cm} & Advisor: & Vương Bá Thịnh\\
& Students: & Phan Thanh Tấn - 2213076.\\
\end{tabular}
\end{table}

\begin{center}
{\footnotesize Thành phố Hồ Chí Minh, tháng 2 năm 2025}
\end{center}
\end{titlepage}


%\thispagestyle{empty}

\newpage
\tableofcontents
\newpage

% =============================
% CHAPTER 1: INTRODUCTION
% =============================

\section{Introduction}

Chess is an ancient intellectual game, considered a benchmark for human logical and strategic thinking. In the digital era, building AI systems capable of playing chess at a high level is not only a scientific challenge but also has great practical significance in artificial intelligence (AI) and machine learning research. This project focuses on developing a chess AI system with multiple agents using various algorithms, including modern machine learning approaches.

% =============================
% CHAPTER 2: THEORETICAL BACKGROUND
% =============================

\section{Theoretical Background}

\subsection{Overview of AI and Machine Learning}
Artificial Intelligence (AI) is the field of study focused on enabling computers to perform tasks that require human intelligence. Machine Learning is a branch of AI that allows computers to learn from data to improve performance without explicit programming.

\subsection{Search Algorithms in Chess}

\subsubsection{Random Agent}
\textbf{Principle:} The Random Agent selects a move at random from the set of all legal moves. It does not use any evaluation or lookahead, making it the simplest possible agent.

\textbf{Advantages:} Extremely fast, useful as a baseline for comparison.

\textbf{Disadvantages:} No strategy, performs poorly against any non-random opponent.

\textbf{Example code:}
\begin{lstlisting}[language=Python, caption=Random Agent]
def get_move(self, board):
    import random
    return random.choice(list(board.legal_moves))
\end{lstlisting}

\subsubsection{Minimax Agent}
\textbf{Principle:} The Minimax algorithm explores the game tree to a fixed depth, assuming both players play optimally. At each node, it recursively evaluates the best achievable outcome for the maximizing and minimizing player.

\textbf{Evaluation Function:}
A common evaluation function is the material count:
\[
Score = 1(P) + 3(N) + 3(B) + 5(R) + 9(Q)
\]
where $P, N, B, R, Q$ are the number of pawns, knights, bishops, rooks, and queens for each side. More advanced functions add piece-square tables, king safety, mobility, etc.

\textbf{Step-by-step Example:}
Suppose White can play $e4$ or $d4$. For each move, the algorithm simulates the move, then recursively evaluates all Black's responses, and so on, up to the given depth. At the leaves, the evaluation function is called. The best move is chosen by back-propagating the scores.

\textbf{Iterative Deepening:}
Instead of searching to a fixed depth in one go, iterative deepening searches to depth 1, then 2, then 3, etc., using the result of the previous iteration to improve move ordering. This allows for time control and better pruning.

\textbf{Transposition Table:}
A hash table that stores previously evaluated positions to avoid redundant computation. This is crucial in chess, where the same position can be reached by different move orders.

\textbf{Advanced Example:}
\begin{lstlisting}[language=Python, caption=Minimax with Transposition Table]
def minimax_tt(board, depth, alpha, beta, maximizing, tt):
    key = board.fen()
    if key in tt:
        return tt[key]
    if depth == 0 or board.is_game_over():
        val = evaluate(board)
        tt[key] = val
        return val
    if maximizing:
        value = -float('inf')
        for move in board.legal_moves:
            board.push(move)
            value = max(value, minimax_tt(board, depth-1, alpha, beta, False, tt))
            board.pop()
            alpha = max(alpha, value)
            if alpha >= beta:
                break
        tt[key] = value
        return value
    else:
        value = float('inf')
        for move in board.legal_moves:
            board.push(move)
            value = min(value, minimax_tt(board, depth-1, alpha, beta, True, tt))
            board.pop()
            beta = min(beta, value)
            if beta <= alpha:
                break
        tt[key] = value
        return value
\end{lstlisting}

\textbf{Strengths:}
\begin{itemize}
    \item Guarantees optimal play if search is deep enough and evaluation is accurate.
    \item Deterministic and explainable.
    \item Can be enhanced with iterative deepening, transposition tables, move ordering.
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Exponential growth in computation with depth.
    \item Relies heavily on evaluation function quality.
    \item Not practical for very deep searches in chess without further enhancements.
\end{itemize}

\subsubsection{Monte Carlo Tree Search (MCTS) Agent}
\textbf{Principle:} MCTS builds a search tree by running many random simulations (playouts) from the current position. It balances exploration and exploitation using the Upper Confidence Bound (UCB1) formula.

\textbf{Detailed Phases:}
\begin{enumerate}
    \item \textbf{Selection:} Starting from the root, recursively select child nodes with the highest UCB1 value until a node with untried moves is reached. This phase balances between exploiting known good moves and exploring new ones.
    \item \textbf{Expansion:} If the selected node has untried moves, one is chosen at random and a new child node is added to the tree.
    \item \textbf{Simulation (Rollout):} From the new node, play random (or semi-random) moves until the game ends. The result (win/loss/draw) is used as the value for this simulation.
    \item \textbf{Backpropagation:} The result is propagated back up the tree, updating the statistics (wins, visits) for each node along the path.
\end{enumerate}

\textbf{Rollout Policy:}
The simplest rollout policy is to play random moves. Stronger policies use shallow evaluation or heuristics to guide the simulation, improving the quality of the value estimate.

\textbf{UCB1 Parameter Analysis:}
The exploration parameter $c$ in UCB1 controls the trade-off between exploration and exploitation. Higher $c$ encourages more exploration, lower $c$ focuses on exploitation. Tuning $c$ is important for performance.

\textbf{Bias and Exploration:}
If the rollout policy is biased (e.g., always captures), the MCTS may overestimate certain lines. Purely random rollouts can be weak, but too much bias can reduce exploration.

\textbf{Modern Variants:}
\begin{itemize}
    \item \textbf{AlphaZero/Leela:} Use a neural network to provide both value and policy priors, replacing random rollouts with network evaluation (PUCT formula).
    \item \textbf{RAVE:} Shares statistics between similar moves to speed up learning.
\end{itemize}

\textbf{Strengths:}
\begin{itemize}
    \item Scales well to complex games.
    \item Does not require a handcrafted evaluation function.
    \item Can be combined with neural networks for even stronger play.
    \item Flexible time management: can stop at any time and return the best move found so far.
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Random playouts may not reflect true position value.
    \item Requires many simulations for strong play.
    \item Less explainable than minimax.
    \item Sensitive to rollout policy and UCB1 parameter.
\end{itemize}

\subsubsection{Deep Learning Agent}
\textbf{Principle:} The Deep Learning Agent uses a neural network to evaluate board positions or predict move probabilities. The network is trained on a large dataset of chess games.

\paragraph{Board Encoding}
A crucial step is converting the chess board into a numerical format suitable for neural networks. Two common approaches:
\begin{itemize}
    \item \textbf{Bitboard:} Each piece type and color is represented as a separate 64-bit vector. For example, all white pawns are encoded as a 64-bit array with 1s at pawn locations.
    \item \textbf{Planes:} The board is encoded as a $8 \times 8 \times 12$ tensor (12 planes for 6 piece types x 2 colors). Each plane is a binary matrix indicating the presence of a specific piece type and color.
\end{itemize}

\textbf{Example: Plane Encoding}
\begin{verbatim}
Plane 0: White Pawns
Plane 1: White Knights
...
Plane 5: White King
Plane 6: Black Pawns
...
Plane 11: Black King
\end{verbatim}
A white pawn on e2 is encoded as 1 at (4,1,0) (using 0-based indexing).

\paragraph{Additional Features}
Other features such as castling rights, move count, repetition, and side to move can be encoded as extra channels or appended to the input vector.

\paragraph{Data Augmentation}
To improve generalization, board positions can be rotated (by 180 degrees), mirrored (left-right), or color-inverted. This increases the effective size of the training set and helps the network learn symmetries of chess.

\paragraph{Regularization}
To prevent overfitting, techniques such as dropout (randomly zeroing activations), weight decay (L2 regularization), and batch normalization are used. These help the network generalize better to unseen positions.

\paragraph{Network Architectures}
\begin{itemize}
    \item \textbf{MLP (Multi-Layer Perceptron):} Flattens the board and passes through several fully connected layers. Fast but ignores spatial structure.
    \item \textbf{CNN (Convolutional Neural Network):} Uses convolutional layers to capture local patterns and spatial relationships between pieces. Widely used in modern chess engines.
    \item \textbf{Residual Networks:} Allow very deep networks by adding skip connections (residual blocks). Used in AlphaZero/Leela Chess Zero.
\end{itemize}

\textbf{Example: Residual Block in PyTorch}
\begin{lstlisting}[language=Python, caption=Residual Block]
import torch.nn as nn
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
    def forward(self, x):
        residual = x
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual
        return torch.relu(out)
\end{lstlisting}

\paragraph{Loss Functions}
\begin{itemize}
    \item \textbf{Regression:} Mean squared error for value prediction (e.g., win probability).
    \item \textbf{Classification:} Cross-entropy loss for move prediction (policy head).
    \item \textbf{Combined:} AlphaZero uses a sum of value and policy losses: $L = (z - v)^2 - \pi^T \log p$, where $z$ is the game result, $v$ is the value head output, $\pi$ is the target policy, $p$ is the predicted policy.
\end{itemize}

\paragraph{Training Pipeline}
\begin{enumerate}
    \item Extract positions and results from PGN game files or self-play.
    \item Encode positions as tensors, results as targets (value, policy).
    \item Split into training/validation sets.
    \item Train using mini-batch gradient descent (e.g., Adam), monitor loss and accuracy.
    \item Evaluate on validation set, tune hyperparameters (learning rate, batch size, architecture).
    \item Optionally, use self-play to generate new data and continue training (reinforcement learning).
\end{enumerate}

\textbf{Example: Full Training Loop}
\begin{lstlisting}[language=Python, caption=Training Loop]
for epoch in range(num_epochs):
    for batch in dataloader:
        boards, targets = batch
        optimizer.zero_grad()
        outputs = model(boards)
        loss = loss_fn(outputs, targets)
        loss.backward()
        optimizer.step()
    # Validation, logging, checkpointing...
\end{lstlisting}

\paragraph{Practical Issues}
\begin{itemize}
    \item \textbf{Data Quality:} Training on high-level games (e.g., grandmaster PGNs) yields better evaluation than random games.
    \item \textbf{Overfitting:} Monitor validation loss, use regularization and augmentation.
    \item \textbf{Hardware:} Training deep networks for chess is computationally expensive and often requires GPUs.
    \item \textbf{Interpretability:} Neural networks are black boxes; it is hard to explain why a move is chosen.
\end{itemize}

\paragraph{Modern Approaches}
\begin{itemize}
    \item \textbf{AlphaZero/Leela Chess Zero:} Use deep residual networks, self-play reinforcement learning, and MCTS guided by neural network policy and value heads.
    \item \textbf{Hybrid Models:} Combine classical search (minimax, MCTS) with neural network evaluation for best results.
\end{itemize}

\paragraph{Comparison with Classical Methods}
\begin{itemize}
    \item Deep learning agents can learn complex patterns and generalize to unseen positions, but require much more data and compute.
    \item Classical methods are more interpretable and require less data, but are limited by the quality of the evaluation function.
    \item The strongest modern engines combine both approaches.
\end{itemize}

\subsection{Elo Rating System}
The Elo rating system is a widely used method for calculating the relative skill levels of players in zero-sum games such as chess. It is also used to evaluate the strength of AI agents by simulating matches between them.

\paragraph{Principle}
Each player (or agent) is assigned a numerical rating. After each game, the ratings are updated based on the game result and the expected outcome. The system assumes that the difference in ratings between two players predicts the expected score.

\paragraph{Expected Score}
Given two players with ratings $R_A$ and $R_B$, the expected score for player A is:
\[
E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}
\]
Similarly, the expected score for player B is $E_B = 1 - E_A$.

\paragraph{Rating Update}
After a game, the ratings are updated as follows:
\[
R_A' = R_A + K (S_A - E_A)
\]
where:
\begin{itemize}
    \item $R_A'$: new rating for player A
    \item $K$: development coefficient (K-factor), controls sensitivity to new results (typical values: 10--40)
    \item $S_A$: actual score (1 for win, 0.5 for draw, 0 for loss)
    \item $E_A$: expected score
\end{itemize}

\paragraph{Example Calculation}
Suppose agent A has $R_A = 1600$, agent B has $R_B = 1400$, $K = 32$.
\begin{itemize}
    \item $E_A = \frac{1}{1 + 10^{(1400-1600)/400}} = \frac{1}{1 + 10^{-0.5}} \approx 0.76$
    \item If A wins ($S_A = 1$):
    \[
    R_A' = 1600 + 32 \times (1 - 0.76) = 1600 + 7.68 = 1607.68
    \]
    \item If A loses ($S_A = 0$):
    \[
    R_A' = 1600 + 32 \times (0 - 0.76) = 1600 - 24.32 = 1575.68
    \]
    \item If draw ($S_A = 0.5$):
    \[
    R_A' = 1600 + 32 \times (0.5 - 0.76) = 1600 - 8.32 = 1591.68
    \]
\end{itemize}

\paragraph{K-factor}
The K-factor determines how much a single game affects the rating. Higher K means ratings change faster (useful for new players/agents), lower K means more stability (for established ratings).

\paragraph{Interpretation}
- A difference of 400 points means the higher-rated player is expected to score about 10 times more than the lower-rated player.
- Elo ratings are relative: only differences matter, not absolute values.
- In AI evaluation, Elo allows direct comparison of agent strength across many games and opponents.

\paragraph{Variants}
- \textbf{Glicko}: Adds rating deviation (uncertainty) to model confidence.
- \textbf{TrueSkill}: Used by Microsoft, models multiplayer and draws more flexibly.
- \textbf{Bayesian Elo}: Used in computer chess tournaments for more robust estimation.

\paragraph{Application in AI Agent Evaluation}
In this project, Elo ratings are used to compare agents by running round-robin tournaments. After each game, both agents' ratings are updated. The final Elo scores reflect the relative strength of each agent.

\paragraph{Example: Python Code for Elo Update}
\begin{lstlisting}[language=Python, caption=Elo Rating Update]
def expected_score(rating_a, rating_b):
    return 1 / (1 + 10 ** ((rating_b - rating_a) / 400))

def update_elo(rating_a, rating_b, score_a, k=32):
    exp_a = expected_score(rating_a, rating_b)
    new_a = rating_a + k * (score_a - exp_a)
    return new_a
\end{lstlisting}

% =============================
% CHAPTER 3: PROBLEM ANALYSIS
% =============================

\section{Problem Analysis}

\subsection{Motivation}
Chess is a classic testbed for artificial intelligence research due to its well-defined rules, immense complexity, and rich history of human and computer competition. Building a strong chess AI system demonstrates the ability to combine search, evaluation, and learning techniques, and provides a benchmark for comparing different AI approaches. The recent success of deep learning and reinforcement learning in chess (e.g., AlphaZero) motivates the integration of modern machine learning into traditional search-based agents.

\subsection{Objectives}
The main objectives of this project are:
\begin{itemize}
    \item Design and implement a chess AI system with multiple agent types (Random, Minimax, MCTS, Deep Learning).
    \item Provide a modern, user-friendly interface for playing and evaluating agents.
    \item Compare and analyze the performance of different agents using objective metrics (win rate, Elo rating, computation time).
    \item Explore the impact of machine learning on chess agent strength.
\end{itemize}

\subsection{Functional Requirements}
\begin{itemize}
    \item Playable chess game with full rules (legal moves, check, checkmate, stalemate, castling, en passant, promotion).
    \item Support for Player vs AI, AI vs AI, and Player vs Player modes.
    \item Agent selection for both sides, with adjustable difficulty and agent type.
    \item Display of move history, captured pieces, and game status.
    \item Agent evaluation and ranking (Elo system, win rate, average move time).
    \item Undo/redo moves, start new game, and resign options.
    \item Responsive, modern graphical user interface.
\end{itemize}

\subsection{Non-Functional Requirements}
\begin{itemize}
    \item Cross-platform compatibility (Windows, Linux, MacOS).
    \item Fast response time for user moves and agent calculations.
    \item Modular, extensible codebase for adding new agents or features.
    \item Robust error handling and user feedback.
    \item Clear documentation and usage instructions.
\end{itemize}

\subsection{Agent Analysis}
The system includes several types of agents, each with distinct characteristics:
\begin{itemize}
    \item \textbf{Random Agent:} Baseline agent, selects moves randomly. Useful for testing and benchmarking.
    \item \textbf{Minimax Agent:} Uses fixed-depth search and material evaluation. Strength depends on search depth and evaluation function.
    \item \textbf{MCTS Agent:} Uses Monte Carlo Tree Search, simulates many games to select optimal move.
    \item \textbf{Deep Learning Agent:} Uses a neural network trained on chess positions to evaluate moves. Can generalize from data and learn complex patterns.
\end{itemize}

\paragraph{Agent Comparison Table}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Agent} & \textbf{Strategy} & \textbf{Strength} & \textbf{Speed} & \textbf{Adaptivity} \\
        \hline
        Random & None & Very Low & Very Fast & None \\
        Minimax & Search + Eval & Low--Medium & Medium & None \\
        MCTS & Simulation & Medium--High & Slow--Medium & None \\
        Deep Learning & Learned Eval & Medium--High & Fast--Medium & High \\
        \hline
    \end{tabular}
    \caption{Comparison of agent types}
\end{table}

\subsection{Technical Challenges}
\begin{itemize}
    \item \textbf{State Space Complexity:} Chess has $\sim10^{40}$ possible positions, making exhaustive search infeasible.
    \item \textbf{Move Generation:} Efficiently generating and validating legal moves is critical for performance.
    \item \textbf{Evaluation Function:} Designing or learning a function that accurately assesses board positions is challenging.
    \item \textbf{Time Management:} Balancing search depth/time and move quality, especially for MCTS and Minimax.
    \item \textbf{Machine Learning Integration:} Encoding board states, training data quality, and network design impact agent strength.
    \item \textbf{User Experience:} Providing a responsive, intuitive interface while running complex AI computations.
\end{itemize}

\subsection{Example: Agent Selection Dialog}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/agent_selection.png}
    \caption{Agent selection and configuration dialog}
\end{figure}

% =============================
% CHAPTER 4: DESIGN AND IMPLEMENTATION
% =============================

\section{Design and Implementation}

\subsection{System Architecture}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/architecture.png}
    \caption{System architecture diagram}
\end{figure}

\subsection{Main Components}
\begin{itemize}
    \item \textbf{User Interface (UI):} Built with Pygame, supports intuitive interaction.
    \item \textbf{Game Engine:} Manages rules and board state.
    \item \textbf{Agent:} Various AI algorithms.
    \item \textbf{Evaluation Suite:} Agent evaluation and comparison.
\end{itemize}

\subsection{Agent Details}
\subsubsection{Random Agent}
Selects random legal moves, no strategy.

\subsubsection{Minimax Agent}
Uses minimax algorithm with fixed depth, material evaluation.

\subsubsection{MCTS Agent}
Uses Monte Carlo Tree Search, simulates many games to select optimal move.

\subsubsection{Deep Learning Agent}
Uses neural networks to evaluate board, learns from real game data.

\subsection{User Interface}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/ui.png}
    \caption{Main UI}
\end{figure}

% =============================
% CHAPTER 5: MACHINE LEARNING AGENT
% =============================

\section{Machine Learning Agent}

\subsection{Neural Network Architecture}
The model uses a multi-layer neural network (MLP/CNN), input is board state, output is position evaluation or move probabilities.

\subsection{Training Process}
Training data is taken from real chess games, preprocessed, split into train/test sets, trained with appropriate loss function.

\subsection{Training Results}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Epoch} & \textbf{Train Loss} & \textbf{Test Loss} & \textbf{Accuracy} \\
        \hline
        1 & 0.45 & 0.48 & 72\% \\
        10 & 0.32 & 0.36 & 80\% \\
        20 & 0.28 & 0.31 & 83\% \\
        \hline
    \end{tabular}
    \caption{Model training results}
\end{table}

% =============================
% CHAPTER 6: EXPERIMENTS AND EVALUATION
% =============================

\section{Experiments and Evaluation}

\subsection{Experimental Setup}
Agents were evaluated in both head-to-head and round-robin tournaments. Metrics recorded include win rate, draw rate, average time per move, average moves per game, and Elo rating. The evaluation was performed on a standard PC using the implemented framework.

\subsection{Experimental Results}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Agent} & \textbf{Win} & \textbf{Draw} & \textbf{Loss} & \textbf{Avg Time (s)} & \textbf{Avg Moves} \\
        \hline
        Random & 0 & 4 & 0 & 0.07 & 89.0 \\
        Minimax-2 & 2 & 0 & 2 & 8.17 & 136.0 \\
        MCTS-5s & 0 & 2 & 2 & 330.08 & 132.5 \\
        % Deep Learning agent results can be added here if available
        \hline
    \end{tabular}
    \caption{Head-to-head results vs Random Agent}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Agent} & \textbf{Final Elo Rating} \\
        \hline
        Minimax (d=3) & 546.8 \\
        Random & 521.3 \\
        Minimax (d=2) & 510.0 \\
        Deep Learning & 473.2 \\
        MCTS (1s) & 448.6 \\
        \hline
    \end{tabular}
    \caption{Final Elo ratings after round-robin tournament}
\end{table}

\subsection{Tournament Results}
The round-robin tournament included 5 agents: Random, Minimax (depth 2), Minimax (depth 3), MCTS (1s), and Deep Learning. Each pair played 2 games. The results show that Minimax (depth 3) achieved the highest Elo, followed by Random and Minimax (depth 2). The Deep Learning and MCTS agents performed less strongly in this configuration, possibly due to limited training or short simulation time.

\subsection{Discussion}
The Minimax agent with higher depth consistently outperformed other agents, demonstrating the effectiveness of deeper search in tactical positions. The Random agent, while simple, sometimes achieved surprising results due to the stochastic nature of play. The MCTS agent, with only 1 second per move, struggled to match the performance of deeper Minimax, likely due to insufficient simulations. The Deep Learning agent, although promising in theory, underperformed in this experiment, highlighting the importance of high-quality training data and sufficient model complexity.

\subsection{Limitations and Observations}
\begin{itemize}
    \item \textbf{Computation Time:} MCTS with 5s per move was extremely slow (avg 330s/move), making it impractical for real-time play in this setup.
    \item \textbf{Draw Rate:} Many games ended in draws, especially between agents of similar strength or style.
    \item \textbf{Training Data:} The Deep Learning agent's performance suggests a need for more data or improved architecture.
    \item \textbf{Parameter Sensitivity:} Agent performance is sensitive to depth (Minimax), simulation time (MCTS), and model/training quality (Deep Learning).
\end{itemize}

% =============================
% CHAPTER 7: DISCUSSION
% =============================

\section{Discussion}

\subsection{Analysis of Experimental Results}
The experiments confirm that deeper search (Minimax d=3) yields the strongest play among the tested agents, as reflected in the highest Elo rating (546.8). The Random agent, while not competitive, sometimes benefits from unpredictability. MCTS, with limited simulation time, and the Deep Learning agent, with current training, lag behind in both Elo and win rate.

\subsection{Quantitative Comparison}
\begin{itemize}
    \item \textbf{Minimax (d=3):} Highest Elo, strong tactical play, but slow (up to 129s/game).
    \item \textbf{Random:} Second highest Elo, but only due to upsets and draws; not a strong agent.
    \item \textbf{MCTS (1s):} Low Elo, many draws, very slow with higher simulation time.
    \item \textbf{Deep Learning:} Underperformed, indicating need for more data or better model.
\end{itemize}

\subsection{Lessons and Recommendations}
\begin{itemize}
    \item For practical play, Minimax with moderate depth offers a good balance of strength and speed.
    \item MCTS requires more simulation time or neural network guidance to be competitive.
    \item Deep Learning agents need extensive, high-quality training data and tuning.
    \item Automated Elo evaluation provides clear, objective benchmarks for improvement.
\end{itemize}

% =============================
% CHAPTER 8: CONCLUSION
% =============================

\section{Conclusion}

This project demonstrates that, in the current configuration, classical search-based agents (Minimax) outperform MCTS and Deep Learning agents in both Elo and win rate. The results highlight the importance of search depth, simulation time, and training data quality. While Deep Learning and MCTS have great potential, their effectiveness depends on careful tuning and sufficient resources. The modular platform enables further research and improvement, and future work should focus on enhancing training pipelines, optimizing agent parameters, and integrating hybrid approaches for stronger play.

% =============================
% APPENDIX
% =============================

\section*{Appendix}

\subsection*{Installation and Usage Guide}
\begin{itemize}
    \item Clone repository: \texttt{git clone https://github.com/thanhtan2210/ai-agent-chess}
    \item Install dependencies: \texttt{pip install -r requirements.txt}
    \item Run game: \texttt{python -m src.main}
    \item Evaluate agent: \texttt{python -m src.evaluate\_agents}
\end{itemize}

\subsection*{Code Examples}
\begin{lstlisting}[language=Python, caption=Example: Initialize MCTS Agent]
from src.agents.mcts_agent import MCTSAgent
agent = MCTSAgent(color=chess.WHITE, max_time=5.0)
move = agent.get_move(board)
\end{lstlisting}

\subsection*{UI Screenshots}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/ui.png}
    \caption{Program UI}
\end{figure}

% =============================
% REFERENCES
% =============================

\begin{thebibliography}{9}
\bibitem{chessai}
  David Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm", arXiv:1712.01815
\bibitem{pythonchess}
  python-chess library: \url{https://python-chess.readthedocs.io/}
\bibitem{pygame}
  Pygame library: \url{https://www.pygame.org/}
\bibitem{elo}
  Elo rating system: \url{https://en.wikipedia.org/wiki/Elo_rating_system}
\end{thebibliography}

\end{document}